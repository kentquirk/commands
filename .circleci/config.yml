version: 2.1
docker_cache_key: &docker_cache_key "{{ .Revision }}"
aws_defaults: &aws_defaults
  AWS_DEFAULT_REGION: us-east-1
  ECR_ENDPOINT: 578681496768.dkr.ecr.us-east-1.amazonaws.com

commands:
  notify:
    description: "Notifies the team with a message"
    parameters:
      msg:
        type: string
    steps:
      - run:
          name: send message
          command: |
              # this is necessary to get the environment variables to interpolate properly
              echo "{\"text\":\"<< parameters.msg >>\"}" >parms.txt
              curl -X POST -H 'Content-type: application/json' --data @parms.txt https://hooks.slack.com/services/$SLACK_KEY

  notify_error:
    description: "Notifies the team with a message only when an error occurs"
    parameters:
      msg:
        type: string
    steps:
      - run:
          name: send error message
          command: |
              echo "{\"text\":\"<< parameters.msg >>\"}" >parms.txt
              curl -X POST -H 'Content-type: application/json' --data @parms.txt https://hooks.slack.com/services/$SLACK_KEY
          when: on_fail

  mark_honeycomb:
    description: "Places a marker in the specified honeycomb dataset"
    parameters:
      msg:
        type: string
      dataset:
        type: string
    steps:
      - run:
          name: create marker
          command: |
            echo "{\"message\":\"<< parameters.msg >>\", \"type\":\"deploy\"}" >parms.txt
            cat parms.txt   #just to be sure
            curl -X POST -H "X-Honeycomb-Team: $HONEYCOMB_KEY" -d @parms.txt https://api.honeycomb.io/1/markers/<< parameters.dataset >>

  save_image:
    description: "Saves a docker image to the cache"
    parameters:
      img_name:
        type: string
    steps:
      # bring back a cache if one already exists
      - restore_cache:
          key: *docker_cache_key
      - run:
          name: save docker image to cache
          command: |
            # ensure cache directory
            [ -d /opt/docker-cache ] || mkdir -p /opt/docker-cache
            #
            docker save -o "/opt/docker-cache/<< parameters.img_name >>.tar" "<< parameters.img_name >>"
      # saves this path with this cache key
      - save_cache:
          key: *docker_cache_key
          paths:
            - /opt/docker-cache

  setup:
    description: "These steps should be run before any real ci/cd actions"
    steps:
      # add an ssh key granted with this circleci's settings for this repo
      - add_ssh_keys:
          fingerprints:
            - "7d:f1:8e:9e:99:9a:26:e2:4d:0c:66:f3:d4:74:10:e7"
      - setup_remote_docker:
          docker_layer_caching: false
      - run:
          name: Turn environment variable secrets into files
          command: |
            # install helm certs
            mkdir -p /root/.helm
            echo -e "$helm_ca_pem" > /root/.helm/ca.pem
            echo -e "$helm_cert_pem" > /root/.helm/cert.pem
            echo -e "$helm_key_pem" > /root/.helm/key.pem

            # install kube config certs
            echo -e "$kube_config" > /root/kubeconfig

            # AWS ECR Login
            eval $(aws ecr get-login --no-include-email --region ${AWS_DEFAULT_REGION})

      - run:
          name: Checkout code
          command: |
            if [ -z "$CIRCLE_BRANCH" ] # if CIRCLE_BRANCH is not set, check out CIRCLE_TAG
            then
              echo "CIRCLE_TAG = $CIRCLE_TAG"
              git clone $CIRCLE_REPOSITORY_URL -b $CIRCLE_TAG /commands
            else # use CIRCLE_BRANCH
              echo "CIRCLE_BRANCH = $CIRCLE_BRANCH"
              git clone $CIRCLE_REPOSITORY_URL -b $CIRCLE_BRANCH /commands
            fi

            # add the circle image's known_hosts file to the known_hosts used inside the images
            cp /root/.ssh/known_hosts /commands/deploy/known_hosts

            # install github machine user key
            # This needs to be here. Otherwise git will cowardly refuse to clone into an non-empty directory.
            echo -e "$machine_user_key" > /commands/machine_user_key
      - run:
          name: get git information
          command: |
            cd /commands
            echo 'export VERSION=$(git describe --long --tags --match="v*")' >> $BASH_ENV
            echo 'export SHA=$(git rev-parse --short $CIRCLE_SHA1)' >> $BASH_ENV
            echo 'export CI_USER=$CIRCLE_USERNAME' >> $BASH_ENV
            echo 'export CI_URL=$CIRCLE_BUILD_URL' >> $BASH_ENV
      # docker_cache_key references a cache directory that has docker images in tar balls.
      # the tarballs were saved with a command called save_image, defined in `commands` above.
      - restore_cache:
          key: *docker_cache_key

workflows:
  version: 2
  master-build:
    jobs:
      - test-build:
          filters:
            branches:
              only: /^master$/
      - push:
          requires:
            - test-build
          filters:
            branches:
              only: /^master$/
      - deploy:
          requires:
            - push
          filters:
            branches:
              only: /^master$/
      - integration:
          requires:
            - deploy
          filters:
            branches:
              only: /^master$/
  tagged-build:
    jobs:
      - test-build:
          filters:  # required since `push` has tag filters AND requires `test-build`
            tags:
              ignore: /.*-testnet$|.*-mainnet$/ # only deploy for these tags
            branches:
              ignore: /^master$/
      - push:
          requires:
            - test-build
          filters:  # required since `deploy` has tag filters AND requires `push`
            tags:
              only: /.*-push$|.*-deploy$/
            branches:
              ignore: /.*/
      - deploy:
          requires:
            - push
          filters:
            tags:
              only: /.*-deploy$/
      - deploy-testnet:
          filters:
            tags:
              only: /.*-testnet$/
            branches:
              ignore: /.*/
      - deploy-mainnet:
          filters:
            tags:
              only: /.*-mainnet$/
            branches:
              ignore: /.*/
      - integration:
          requires:
            - deploy
          filters:
            tags:
              only: /.*-deploy$/

general_config: &general_config
    working_directory: /commands
    docker:
        - image: 578681496768.dkr.ecr.us-east-1.amazonaws.com/circle-ci:0.0.9
    environment:
        shell: /bin/bash
        <<: *aws_defaults
        # nodes will be accessible from their release name combined with a number
        # and the subdomain of the ELB. (e.g. devnet-0.api.ndau.tech)
        ELB_SUBDOMAIN: api.ndau.tech
        KUBECONFIG: /root/kubeconfig
        NDAUHOME: /root/.ndau
        S3_NODE_ID_ARCHIVE: sc-nodes-devnet.tgz
        HONEYCOMB_DATASET: "devnet"
        NETWORK_NAME: "devnet" # used for snapshots
        SNAPSHOT_REDIS_TAG: 0.0.1
        CHAOS_TM_TAG: v0.31.0
        CHAOS_NOMS_TAG: 0.0.2
        CHAOS_REDIS_TAG: 0.0.1
        NDAU_TM_TAG: v0.31.0
        NDAU_NOMS_TAG: 0.0.2
        NDAU_REDIS_TAG: 0.0.1

testnet_config: &testnet_config
    working_directory: /commands
    docker:
        - image: 578681496768.dkr.ecr.us-east-1.amazonaws.com/circle-ci:0.0.9
    environment:
        shell: /bin/bash
        <<: *aws_defaults
        # nodes will be accessible from their release name combined with a number
        # and the subdomain of the ELB. (e.g. devnet-0.api.ndau.tech)
        ELB_SUBDOMAIN: api.ndau.tech
        KUBECONFIG: /root/kubeconfig
        S3_DEPLOY_ARCHIVE: testnet.tgz
        HONEYCOMB_DATASET: "testnet"
        NETWORK_NAME: "testnet" # used for snapshots
        SNAPSHOT_REDIS_TAG: 0.0.1
        CHAOS_TM_TAG: v0.31.0
        CHAOS_NOMS_TAG: 0.0.2
        CHAOS_REDIS_TAG: 0.0.1
        NDAU_TM_TAG: v0.31.0
        NDAU_NOMS_TAG: 0.0.2
        NDAU_REDIS_TAG: 0.0.1

mainnet_config: &mainnet_config
    working_directory: /commands
    docker:
        - image: 578681496768.dkr.ecr.us-east-1.amazonaws.com/circle-ci:0.0.9
    environment:
        shell: /bin/bash
        <<: *aws_defaults
        # nodes will be accessible from their release name combined with a number
        # and the subdomain of the ELB. (e.g. node-0.main.ndau.tech)
        ELB_SUBDOMAIN: main.ndau.tech
        KUBECONFIG: /root/kubeconfig
        S3_DEPLOY_ARCHIVE: mainnet.tgz
        HONEYCOMB_DATASET: "mainnet"
        NETWORK_NAME: "mainnet" # used for snapshots
        SNAPSHOT_REDIS_TAG: 0.0.1
        CHAOS_TM_TAG: v0.31.0
        CHAOS_NOMS_TAG: 0.0.2
        CHAOS_REDIS_TAG: 0.0.1
        NDAU_TM_TAG: v0.31.0
        NDAU_NOMS_TAG: 0.0.2
        NDAU_REDIS_TAG: 0.0.1

jobs:
  test-build:
    <<: *general_config
    steps:
      - setup
      - run:
          name: Test and build
          command: |
            # build deps image
            docker build -t deps -f /commands/deploy/deps.docker /commands/

            # this runs a test script within the deps image
            docker run --rm \
                -e CI=true \
                deps \
                /bin/sh /root/test-and-build.sh

      - save_image:
          img_name: deps:latest

  push:
    <<: *general_config
    steps:
      - setup
      - run:
          name: run all sub project docker scripts
          command: |
            find /opt/docker-cache -name "*.tar" -exec docker load -i {} \;
            # reload saved environment variables
            source $BASH_ENV
            # build single container node image
            ./docker/bin/buildimage.sh
            # retag built image
            docker tag ndauimage 578681496768.dkr.ecr.us-east-1.amazonaws.com/sc-node:$SHA
            # push the image to ECR
            docker push 578681496768.dkr.ecr.us-east-1.amazonaws.com/sc-node:$SHA


  deploy:
    <<: *general_config
    steps:
      - setup
      - mark_honeycomb:
          dataset: $HONEYCOMB_DATASET
          msg: Start deploy for build $CIRCLE_BUILD_NUM
      - run:
          name: run the deploy script
          command: |
            # Download node identities
            node_id_dir=/root/node-identities
            mkdir -p "$node_id_dir"
            AWS_ACCESS_KEY_ID=$AWS_DEPLOY_SECRETS_ID \
            AWS_SECRET_ACCESS_KEY=$AWS_DEPLOY_SECRETS_KEY \
              aws s3 cp s3://ndau-deploy-secrets/$S3_NODE_ID_ARCHIVE $node_id_dir/$S3_NODE_ID_ARCHIVE
            (
              cd $node_id_dir
              tar xvf $S3_NODE_ID_ARCHIVE
            )

            # get bash environment variables from previous steps
            source $BASH_ENV

            # Deploy the container to fargate
            APP_BASE_NAME=sc-node \
            NODE_ID=sc-node-0 \
            SNAPSHOT="snapshot-devnet-g" \
            PERSISTENT_PEERS="ca195d91c051d91c451483b08bdea0059e39fd34@100.24.11.77:30051" \
            NODE_IDENTITY=$(base64 $node_id_dir/sc-node-0.tgz) \
              /commands/docker/bin/deploycontainer.sh

      - notify:
          msg: Deploy complete; nodes for $NETWORK_NAME are now starting up.


  deploy-testnet:
    <<: *testnet_config
    steps:
      - setup
      - mark_honeycomb:
          dataset: $HONEYCOMB_DATASET
          msg: Start deploy for build $CIRCLE_BUILD_NUM
      - run:
          name: run the deploy script
          command: |
            # Redeploy nodegroup testnet
            cd /commands

            # Clone the automation repo master branch
            git clone git@github.com:oneiro-ndev/automation.git /root/automation
            ( cd /root/automation && echo "automation repo at $(git rev-parse --short HEAD)" )

            # copy and unzip installation scripts from aws
            mkdir -p /root/nodegroups
            AWS_ACCESS_KEY_ID=$AWS_DEPLOY_SECRETS_ID \
            AWS_SECRET_ACCESS_KEY=$AWS_DEPLOY_SECRETS_KEY \
              aws s3 cp s3://ndau-deploy-secrets/$S3_DEPLOY_ARCHIVE /root/nodegroups/$S3_DEPLOY_ARCHIVE
            (
              cd /root/nodegroups
              tar xvf $S3_DEPLOY_ARCHIVE
            )

            # bring down nodes
            /root/nodegroups/down.sh

            # wait for stragglers to die. pvcs always go down last
            # TODO add some smarter resource detection
            while true; do
              if
                [ "$(kubectl get pvc -l release=testnet-0 | wc -l | tr -d "[:space:]")" == "0" ] &&
                [ "$(kubectl get pvc -l release=testnet-1 | wc -l | tr -d "[:space:]")" == "0" ]; then
                  break
              else
                echo "waiting for testnet to die"
                sleep 1
              fi
            done

            # set SHA to what the logic in the checkout step has determined.
            source $BASH_ENV
            echo "SHA = $SHA"

            # brings nodes back up
            CHAOSNODE_TAG=$SHA \
            NDAUNODE_TAG=$SHA \
            HELM_CHART_PATH=/root/automation/helm/nodegroup \
              /root/nodegroups/up.sh
      - notify:
          msg: Deploy complete; nodes for $NETWORK_NAME are now starting up.

  deploy-mainnet:
    <<: *mainnet_config
    steps:
      - setup
      - mark_honeycomb:
          dataset: $HONEYCOMB_DATASET
          msg: Start deploy for build $CIRCLE_BUILD_NUM
      - run:
          name: run the deploy script
          command: |
            # Redeploy nodegroup mainnet
            cd /commands
            # set SHA to latest master update
            export SHA=$(git rev-parse --short `git ls-remote git@github.com:oneiro-ndev/commands.git master | cut -f 1`)
            echo SHA=$SHA

            # set helm certs to mainnet values
            echo -e "$helm_ca_pem_main" > /root/.helm/ca.pem
            echo -e "$helm_cert_pem_main" > /root/.helm/cert.pem
            echo -e "$helm_key_pem_main" > /root/.helm/key.pem

            # install kube config certs for mainnet
            echo -e "$kube_config_main" > /root/kubeconfig

            # Clone the automation repo master branch
            git clone git@github.com:oneiro-ndev/automation.git /root/automation
            ( cd /root/automation && echo "automation repo at $(git rev-parse --short HEAD)" )

            # copy and unzip installation scripts from aws
            mkdir -p /root/nodegroups
            AWS_ACCESS_KEY_ID=$AWS_DEPLOY_SECRETS_ID \
            AWS_SECRET_ACCESS_KEY=$AWS_DEPLOY_SECRETS_KEY \
              aws s3 cp s3://ndau-deploy-secrets/$S3_DEPLOY_ARCHIVE /root/nodegroups/$S3_DEPLOY_ARCHIVE
            (
              cd /root/nodegroups
              tar xvf $S3_DEPLOY_ARCHIVE
            )

            # bring down nodes
            /root/nodegroups/down.sh

            # wait for stragglers to die. pvcs always go down last
            # TODO add some smarter resource detection
            while true; do
              if
                [ "$(kubectl get pvc -l release=node-0 | wc -l | tr -d "[:space:]")" == "0" ] &&
                [ "$(kubectl get pvc -l release=node-1 | wc -l | tr -d "[:space:]")" == "0" ] &&
                [ "$(kubectl get pvc -l release=node-2 | wc -l | tr -d "[:space:]")" == "0" ] &&
                [ "$(kubectl get pvc -l release=node-3 | wc -l | tr -d "[:space:]")" == "0" ] &&
                [ "$(kubectl get pvc -l release=node-4 | wc -l | tr -d "[:space:]")" == "0" ]; then
                  break
              else
                echo "waiting for mainnet to die"
                sleep 1
              fi
            done

            # brings nodes back up
            # Use $SHA here instead of the automation's git ls-remote method.
            CHAOSNODE_TAG=$SHA \
            NDAUNODE_TAG=$SHA \
            HELM_CHART_PATH=/root/automation/helm/nodegroup \
              /root/nodegroups/up.sh

      - notify:
          msg: Deploy complete; nodes for $NETWORK_NAME are now starting up.

  integration:
    <<: *general_config
    steps:
      - setup
      - run:
          name: Integration tests
          command: |

            # Run integration tests
            # get address and port of devnet0 RPC
            NODE_IP_ADDRESS=$(kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}' | cut -d " " -f1)
            NODE_PORT_0=$(kubectl get service --namespace default -o jsonpath='{.spec.ports[?(@.name=="rpc")].nodePort}' devnet-0-nodegroup-ndau-tendermint-service)
            # get address and port of devnet1 RPC
            NODE_PORT_1=$(kubectl get service --namespace default -o jsonpath='{.spec.ports[?(@.name=="rpc")].nodePort}' devnet-1-nodegroup-ndau-tendermint-service)

            URL_0=http://$NODE_IP_ADDRESS:$NODE_PORT_0/node/status
            URL_1=http://$NODE_IP_ADDRESS:$NODE_PORT_1/node/status

            # curl retry options
            CURL_CONNECT_TIMEOUT=5  # each try waits X seconds
            CURL_RETRY_MAX=50       # retry X many times
            CURL_RETRY_TOTAL=1000   # arbitrary high number, it will timeout first.
            CURL_RETRY_DELAY=10     # try every X seconds
            CURL_TOTAL_TIMEOUT=900  # total seconds before it fails (900s=15m)

            echo "Trying to connect to $URL_0"
            # curl until devnet-0 RPC is up and running, or CURL_TOTAL_TIMEOUT passes
            if curl --connect-timeout $CURL_CONNECT_TIMEOUT \
                --retry-connrefused \
                --max-time $CURL_RETRY_MAX \
                --retry $CURL_RETRY_TOTAL \
                --retry-delay $CURL_RETRY_DELAY \
                --retry-max-time $CURL_TOTAL_TIMEOUT \
                $URL_0; then
                echo "Pinged $URL_0"
            else
                echo "Can't ping $URL_0"
                exit 1
            fi

            echo "Trying to connect to $URL_1"
            # curl until devnet-1 RPC is up and running, or CURL_TOTAL_TIMEOUT passes
            if curl --connect-timeout $CURL_CONNECT_TIMEOUT \
                --retry-connrefused \
                --max-time $CURL_RETRY_MAX \
                --retry $CURL_RETRY_TOTAL \
                --retry-delay $CURL_RETRY_DELAY \
                --retry-max-time $CURL_TOTAL_TIMEOUT \
                $URL_1; then
                echo "Pinged $URL_1"
            else
                echo "Can't ping $URL_1"
                exit 1
            fi

            # ensure go path location
            export GOPATH=/go
            mkdir -p $GOPATH/bin
            ndev_dir=$GOPATH/src/github.com/oneiro-ndev
            mkdir -p $ndev_dir
            cd $ndev_dir

            # install dep
            curl https://raw.githubusercontent.com/golang/dep/master/install.sh | sh

            # install sed?
            apk add sed

            # clone integration tests and ndev dep repos
            git clone git@github.com:oneiro-ndev/integration-tests.git
            ( cd integration-tests && echo "integration-tests repo at $(git rev-parse --short HEAD)" )

            # copy the commands repo and build the ndau tool
            cp -r /commands $ndev_dir
            (
              cd commands
              echo "commands repo at $(git rev-parse --short HEAD)" # for logs
              # TODO setup runs dep ensure. deps has it already and we should copy to speed up the integration test build in setup.sh
              #    something like this: docker cp $(docker images deps -q):/go/src/github.com/oneiro-ndev/commands/vendor $ndev_dir/commands

              # bypass ./setup.sh question about generating the system_vars.toml file
              mkdir -p /root/.localnet/genesis_files
              touch /root/.localnet/genesis_files/system_vars.toml
              ./bin/setup.sh # run setup just for the sake of building
              ./bin/build.sh
            )

            # write the environment variable containing ndautool.toml to a file.
            mkdir -p $NDAUHOME/ndau
            AWS_ACCESS_KEY_ID=$AWS_DEPLOY_SECRETS_ID \
            AWS_SECRET_ACCESS_KEY=$AWS_DEPLOY_SECRETS_KEY \
              aws s3 cp s3://ndau-deploy-secrets/$NETWORK_NAME-ndautool.toml $NDAUHOME/ndau/ndautool.toml

            cd integration-tests

            # run tests
            pipenv sync
            pipenv run pytest --net=devnet --ndauapi=https://devnet-0.api.ndau.tech
