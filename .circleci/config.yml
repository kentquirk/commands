version: 2.1
aws_defaults: &aws_defaults
  AWS_DEFAULT_REGION: us-east-1
  ECR_ENDPOINT: 578681496768.dkr.ecr.us-east-1.amazonaws.com

commands:
  notify:
    description: "Notifies the team with a message"
    parameters:
      msg:
        type: string
    steps:
      - run:
          name: send message
          command: |
              # this is necessary to get the environment variables to interpolate properly
              echo "{\"text\":\"<< parameters.msg >>\"}" >parms.txt
              curl -X POST -H 'Content-type: application/json' --data @parms.txt https://hooks.slack.com/services/$SLACK_KEY

  notify_error:
    description: "Notifies the team with a message only when an error occurs"
    parameters:
      msg:
        type: string
    steps:
      - run:
          name: send error message
          command: |
              echo "{\"text\":\"<< parameters.msg >>\"}" >parms.txt
              curl -X POST -H 'Content-type: application/json' --data @parms.txt https://hooks.slack.com/services/$SLACK_KEY
          when: on_fail

  mark_honeycomb:
    description: "Places a marker in the specified honeycomb dataset"
    parameters:
      msg:
        type: string
      dataset:
        type: string
    steps:
      - run:
          name: create marker
          command: |
            echo "{\"message\":\"<< parameters.msg >>\", \"type\":\"deploy\"}" >parms.txt
            cat parms.txt   #just to be sure
            curl -X POST -H "X-Honeycomb-Team: $HONEYCOMB_KEY" -d @parms.txt https://api.honeycomb.io/1/markers/<< parameters.dataset >>

  save_image:
    description: "Saves a docker image to the cache"
    parameters:
      img_name:
        type: string
      key:
        type: string
    steps:
      # bring back a cache if one already exists
      - restore_cache:
          key: << parameters.key >>
      - run:
          name: save docker image to cache
          command: |
            # ensure cache directory
            [ -d /opt/docker-cache ] || mkdir -p /opt/docker-cache
            #
            docker save -o "/opt/docker-cache/<< parameters.img_name >>.tar" "<< parameters.img_name >>"
      # saves this path with this cache key
      - save_cache:
          key: << parameters.key >>
          paths:
            - /opt/docker-cache

  setup:
    description: "These steps should be run before any real ci/cd actions"
    steps:
      # add an ssh key granted with this circleci's settings for this repo
      - add_ssh_keys:
          fingerprints:
            - "7d:f1:8e:9e:99:9a:26:e2:4d:0c:66:f3:d4:74:10:e7"
      - setup_remote_docker:
          docker_layer_caching: false
      - run:
          name: AWS ECR Login
          command: eval $(aws ecr get-login --no-include-email --region ${AWS_DEFAULT_REGION})

      - run:
          name: Checkout code
          command: |
            if [ -z "$CIRCLE_BRANCH" ] # if CIRCLE_BRANCH is not set, check out CIRCLE_TAG
            then
              echo "CIRCLE_TAG = $CIRCLE_TAG"
              git clone $CIRCLE_REPOSITORY_URL -b $CIRCLE_TAG /commands
            else # use CIRCLE_BRANCH
              echo "CIRCLE_BRANCH = $CIRCLE_BRANCH"
              git clone $CIRCLE_REPOSITORY_URL -b $CIRCLE_BRANCH /commands
            fi

            # add the circle image's known_hosts file to the known_hosts used inside the images
            cp /root/.ssh/known_hosts /commands/deploy/known_hosts

            # install github machine user key
            # This needs to be here. Otherwise git will cowardly refuse to clone into an non-empty directory.
            echo -e "$machine_user_key" > /commands/machine_user_key

      - run:
          name: get git information
          command: |
            cd /commands
            echo 'export VERSION=$(git describe --long --tags --match="v*")' >> $BASH_ENV
            echo 'export SHA=$(git rev-parse --short $CIRCLE_SHA1)' >> $BASH_ENV
            echo 'export CI_USER=$CIRCLE_USERNAME' >> $BASH_ENV
            echo 'export CI_URL=$CIRCLE_BUILD_URL' >> $BASH_ENV

  setup_ecs_and_identities:
    description: |
      Downloads ecs, configures it and downloads node identities.
      This should be run before any deploy steps.
    steps:
      - run:
          name: download and configure ecs-cli
          command: |
            # download ecs cli
            curl -o /usr/local/bin/ecs-cli https://s3.amazonaws.com/amazon-ecs-cli/ecs-cli-linux-amd64-latest && \
            chmod +x /usr/local/bin/ecs-cli

            # configure ecs
            ecs-cli configure profile --access-key "$AWS_ACCESS_KEY_ID" --secret-key "$AWS_SECRET_ACCESS_KEY" --profile-name default
            ecs-cli configure --cluster "$CLUSTER_NAME" --region "$AWS_DEFAULT_REGION" --config-name "$CLUSTER_NAME"
      - run:
          name: Download node identities
          command: |
            S3_NODE_ID_ARCHIVE="node-identities-${NETWORK_NAME}.tgz"
            AWS_ACCESS_KEY_ID=$AWS_DEPLOY_SECRETS_ID \
            AWS_SECRET_ACCESS_KEY=$AWS_DEPLOY_SECRETS_KEY \
              aws s3 cp "s3://ndau-deploy-secrets/$S3_NODE_ID_ARCHIVE" "./$S3_NODE_ID_ARCHIVE"
            mkdir -p /root/node-identities
            tar xzvf "./$S3_NODE_ID_ARCHIVE" -C /root/node-identities

  deploy-net:
    description: Deploy using ECS
    steps:
      - setup
      - setup_ecs_and_identities
      - mark_honeycomb:
          dataset: $HONEYCOMB_DATASET
          msg: Start deploy for build $CIRCLE_BUILD_NUM
      - ecs_deploy
      - notify:
          msg: Deploy complete; nodes for $NETWORK_NAME are now starting up.

  ecs_deploy:
    description: Deploy using ECS
    steps:
      - run:
          name: deploy
          command: |

            # get bash environment variables from previous steps
            source $BASH_ENV

            for i in $( seq 0 9 ); do # automatically deploy up to 10 nodes
              echo "Attempting node: $i"
              if [ -f  "/root/node-identities/node-identity-$i.tgz" ]; then
                echo "Found node identity at: /root/node-identities/node-identity-$i.tgz"
                /commands/deploy/deploy-node.sh $i $NETWORK_NAME /root/node-identities &
              fi # no else break in case some nodes are updated and others are not
            done

            wait


workflows:
  version: 2
  master-build:
    jobs:
      - build-deps:
          filters:
            branches:
              only: /^master$/
      - test:
          filters:
            branches:
              only: /^master$/
          requires:
            - build-deps
      - build:
          filters:
            branches:
              only: /^master$/
          requires:
            - build-deps
      - push:
          requires:
            - test
            - build
          filters:
            branches:
              only: /^master$/
      - deploy-devnet:
          requires:
            - push
          filters:
            branches:
              only: /^master$/
      - integration:
          requires:
            - deploy-devnet
          filters:
            branches:
              only: /^master$/
  tagged-build:
    jobs:
      - build-deps:
          filters:  # required since `push` has tag filters AND requires `test, build`
            tags:
              ignore: /.*-testnet$|.*-mainnet$/ # only run job for these tags
            branches:
              ignore: /^master$/
      - test:
          filters:  # required since `push` has tag filters AND requires `test, build`
            tags:
              ignore: /.*-testnet$|.*-mainnet$/ # only run job for these tags
            branches:
              ignore: /^master$/
          requires:
            - build-deps
      - build:
          filters:  # required since `push` has tag filters AND requires `test-build`
            tags:
              ignore: /.*-testnet$|.*-mainnet$/ # only run job for these tags
            branches:
              ignore: /^master$/
          requires:
            - build-deps
      - push:
          requires:
            - test
            - build
          filters:  # required since `deploy` has tag filters AND requires `push`
            tags:
              only: /.*-push$|.*-deploy$/
            branches:
              ignore: /.*/
      - deploy-devnet:
          requires:
            - push
          filters:
            tags:
              only: /.*-deploy$/
      - deploy-testnet:
          filters:
            tags:
              only: /.*-testnet$/
            branches:
              ignore: /.*/
      - deploy-mainnet:
          filters:
            tags:
              only: /.*-mainnet$/
            branches:
              ignore: /.*/
      - integration:
          requires:
            - deploy-devnet
          filters:
            tags:
              only: /.*-deploy$/

general_config: &general_config
    working_directory: /commands
    docker:
        - image: 578681496768.dkr.ecr.us-east-1.amazonaws.com/circle-ci:0.0.9
    environment:
        shell: /bin/bash
        <<: *aws_defaults
        NDAUHOME: /root/.ndau
        NETWORK_NAME: "devnet" # used for snapshots
        PERSISTENT_PEERS: 6cc80202fdfcc05ab8e1e2dbff6053f83ad1026e@p2p.ndau.tech:30200,91a090c05db0d49b2d09d879f4b513ec450fc3b6@p2p.ndau.tech:30201,fa77fe34ad9e315fb0104686ee3d22dfe4e4e126@p2p.ndau.tech:30202,6ddf04b622c63875452456458164bf4f0ec21206@p2p.ndau.tech:30203,a040a64fd4a5a2f77148ca2190784216334a928b@p2p.ndau.tech:30204 # devnet
        CLUSTER_NAME: sc-node-cluster
        SNAPSHOT_URL: https://s3.amazonaws.com/ndau-snapshots/snapshot-testnet-27.tgz

testnet_config: &testnet_config
    working_directory: /commands
    docker:
        - image: 578681496768.dkr.ecr.us-east-1.amazonaws.com/circle-ci:0.0.9
    environment:
        shell: /bin/bash
        <<: *aws_defaults
        NETWORK_NAME: "testnet" # used for snapshots
        PERSISTENT_PEERS: 4a57ac1108374b4687bdcc4cb46948292f4578ac@p2p.ndau.tech:31200,caba9ed0805e7c54106d55675525b45227775ad0@p2p.ndau.tech:31201,38cf3ba0a64ea5f8d7338eb3729a19eafe0f5dfb@p2p.ndau.tech:31202,804aa9f0595ca28aede5b958b51805ad94814074@p2p.ndau.tech:31203,91d07344fda8e252e5e97de8936819acaded8767@p2p.ndau.tech:31204 # testnet
        CLUSTER_NAME: sc-node-cluster
        SNAPSHOT_URL: https://s3.amazonaws.com/ndau-snapshots/snapshot-testnet-27.tgz

mainnet_config: &mainnet_config
    working_directory: /commands
    docker:
        - image: 578681496768.dkr.ecr.us-east-1.amazonaws.com/circle-ci:0.0.9
    environment:
        shell: /bin/bash
        <<: *aws_defaults
        # nodes will be accessible from their release name combined with a number
        # and the subdomain of the ELB. (e.g. node-0.main.ndau.tech)
        S3_DEPLOY_ARCHIVE: mainnet.tgz
        HONEYCOMB_DATASET: "mainnet"
        NETWORK_NAME: "mainnet" # used for snapshots
        PERSISTENT_PEERS: d685921874900a52d811477f0e2af5b0690f408d@p2p.ndau.tech:32200,4ac831cde4d7dc1b321d46ab58783f9e732f146a@p2p.ndau.tech:32201,4c761de823793b702345098c0ba652f5df3a0530@p2p.ndau.tech:32202,5de3bd4482b9a75e10c075eef34c7defe395312a@p2p.ndau.tech:32203,77415932068e690d608e2235841120f8e138c7cb@p2p.ndau.tech:32204 # mainnet
        CLUSTER_NAME: sc-node-cluster
        SNAPSHOT_URL: https://s3.amazonaws.com/ndau-snapshots/snapshot-mainnet-1.tgz

jobs:
  build-deps:
    <<: *general_config
    steps:
      - setup
      - run:
          name: build-deps
          command: |
            # build deps image
            docker build -t deps -f /commands/deploy/deps.docker /commands/
      - save_image:
          img_name: deps:latest
          key: "{{ .Revision }}-deps"

  test:
    <<: *general_config
    steps:
      - setup
      - restore_cache:
          key: "{{ .Revision }}-deps"
      - run:
          name: Test
          command: |
            # load previously cached containers
            find /opt/docker-cache -name "*.tar" -exec docker load -i {} \;

            # this runs a test script within the deps image
            docker run --rm \
                -e CI=true \
                deps \
                /bin/sh /root/test.sh

  build:
    <<: *general_config
    steps:
      - setup
      - restore_cache:
          key: "{{ .Revision }}-deps"
      - run:
          name: Build
          command: |
            # load previously cached containers
            find /opt/docker-cache -name "*.tar" -exec docker load -i {} \;

            # this runs a build script within the deps image
            docker run \
                -e CI=true \
                deps \
                /bin/sh /root/build.sh

  push:
    <<: *general_config
    steps:
      - setup
      - run:
          name: build the single container image
          command: |
            # TODO let buildimage.sh use the already fetched vendor dir

            # reload saved environment variables
            source $BASH_ENV
            # build single container node image
            ./docker/bin/buildimage.sh nosave
            # retag built image
            docker tag ndauimage 578681496768.dkr.ecr.us-east-1.amazonaws.com/sc-node:$SHA
            # push the image to ECR
            docker push 578681496768.dkr.ecr.us-east-1.amazonaws.com/sc-node:$SHA


  deploy-devnet:
    <<: *general_config
    steps:
      - deploy-net
  deploy-testnet:
    <<: *testnet_config
    steps:
      - deploy-net
  deploy-mainnet:
    <<: *mainnet_config
    steps:
      - deploy-net

  integration:
    <<: *general_config
    steps:
      - setup
      - restore_cache:
          key: "{{ .Revision }}-deps"
      - run:
          name: Integration tests
          command: |
            # load previously cached containers
            find /opt/docker-cache -name "*.tar" -exec docker load -i {} \;

            # ensure go path location
            export GOPATH=/go
            mkdir -p $GOPATH/bin
            ndev_dir=$GOPATH/src/github.com/oneiro-ndev
            mkdir -p $ndev_dir
            cd $ndev_dir

            # install dep
            curl https://raw.githubusercontent.com/golang/dep/master/install.sh | sh

            # install sed?
            apk add sed

            # clone integration tests and ndev dep repos
            git clone git@github.com:oneiro-ndev/integration-tests.git
            ( cd integration-tests && echo "integration-tests repo at $(git rev-parse --short HEAD)" )

            # build the ndau tool
            docker run deps /bin/sh -c 'go build -o /root/ndau /go/src/github.com/oneiro-ndev/commands/cmd/ndau'
            container_id=$(docker ps -alq 2>&1)
            docker commit "$container_id" temp:latest
            docker run -d temp:latest /bin/sh -c 'sleep 60'
            container_id=$(docker ps -alq 2>&1)
            mkdir -p $ndev_dir/commands
            docker cp $container_id:/root/ndau $ndev_dir/commands/

            # write ndautool from bucket to a file.
            mkdir -p $NDAUHOME/ndau
            AWS_ACCESS_KEY_ID=$AWS_DEPLOY_SECRETS_ID \
            AWS_SECRET_ACCESS_KEY=$AWS_DEPLOY_SECRETS_KEY \
              aws s3 cp s3://ndau-deploy-secrets/$NETWORK_NAME-ndautool.toml $NDAUHOME/ndau/ndautool.toml

            cd integration-tests

            # sync pipenv
            pipenv sync

            # run tests
            NODE_ADDRESS=https://api.ndau.tech \
            NODE_0_RPC=30100 \
            NODE_1_RPC=30101 \
              pipenv run pytest -v --net=devnet --ndauapi=https://api.ndau.tech:30300
